# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ex_i-KRvuwsSKKJzdyNutvyit9BBqsGM
"""

#Code needed to Mount the Google Drive to access CSV Files
# from google.colab import drive
# drive.mount('/content/drive')

#Import Librarys
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from datetime import datetime
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score,f1_score
from sklearn import preprocessing
from sklearn.dummy import DummyClassifier

#Read in data to CSV
df = pd.read_csv("db_data_station_25.csv")
#Allows us to drop the Index Value which has no purpose for our Machine Learning Models
df = df.drop(df.columns[0], axis=1)
#Print the head of the dataset to check it inputted correctly
print (df.head())
#Print length of dataset
print (len(df))
#Convert the dataset to floats
df = df.iloc[:,:].astype('float')

#Create a DataFrame that only contains values when the time is 9am
df_nine_am = df[df.TIME == 9]
#Print the first few rows and length of the dataframe
print (df_nine_am.head())
print (len(df_nine_am))

#Extract the specific values from the DataFrame into a Numpy Array
Date = np.array(df_nine_am.iloc[:,0])
Time = np.array(df_nine_am.iloc[:,1])
Month = np.array(df_nine_am.iloc[:,2])
Temp = np.array(df_nine_am.iloc[:,3])
Rain = np.array(df_nine_am.iloc[:,4])
Bikes = np.array(df_nine_am.iloc[:,5])
#This allows us to be more specific about the different columns that get used for our model
#If we decide not to use the 'Month' colunn for example, we can choose to remove it from the X variable stack
X = np.column_stack((Date,Month,Temp,Rain))
y = np.array(df_nine_am.iloc[:,6])

print (len(df_nine_am))

"""# **Plot of Days of Week vs Bike**"""

fig = plt.figure()
plt.scatter(Date, Bikes)
plt.xlabel('Day of the week')
plt.ylabel('No. of available bikes')
plt.title('Days of Week compared to number of bikes available')

#Print the number of Unique values in the Y column. Tells you how many classes are present
print(df.iloc[:,6].nunique())

"""# **Split Data and Print Shape**"""

#Print size and shape of data points
print (X.shape)
print (y.shape)

# #Train & Test data shape
# print ("------ Training Data------")
# print ("X_train: " + str(X_train.shape))
# print ("Y_train: " + str(y_train.shape))
# print ("------ Test Data------")
# print ("X_test: " + str(X_test.shape))
# print ("Y_test: " + str(y_test.shape))

"""# **Normalise and Scale Data**"""

MinMaxScaler = preprocessing.MinMaxScaler()
Scaled_data = MinMaxScaler.fit_transform(X)
data = pd.DataFrame(Scaled_data,columns=["DATE", "MONTH","TEMP","RAIN"])
print (data.head)

#Extract the Column values into a stack. The Scaled data is converted back into
#Dataframe, therefore they need to be extracted again using numpy

Date = np.array(data.iloc[:,0])
Month = np.array(data.iloc[:,1])
Temp = np.array(data.iloc[:,2])
Rain = np.array(data.iloc[:,3])
X = np.column_stack((Date,Month,Temp,Rain))
print (X)

"""# **Test/Training Data Split**"""

#Split data with 80/20% training/test split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)

"""# **Polynomial Features**"""

#
accuracy_values = [];
poly_range = [1,2,3,4,5,6,7,8,9,10,12,15,20]
for p in poly_range:
    poly = PolynomialFeatures(p)
    features_test = poly.fit_transform(X)
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)
    model = KNeighborsClassifier(n_neighbors = 20, weights = 'uniform') 
    model.fit(X_train, y_train)
    ypred = model.predict(X_test)
    #Get Accuracy values
    accuracy = accuracy_score(y_test,ypred)
    accuracy_values.append(accuracy)

#Plot the figures
plt.figure()
plt.plot(poly_range,accuracy_values,linewidth=3,color='red')
plt.xlabel('Poly Nomial Feature')
plt.title("KNN Cross Validation vs Accuracy with K = 20")
plt.ylabel("Accuracy")
plt.show()
plt.savefig("KNN Cross Validation for PolyNomial Features")

"""Dummy Classifier Report Generation"""

#Split data with 80/20% training/test split
#Classification report code extracted from tutorial - 
#https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)


baseline = DummyClassifier(strategy='uniform')
random = DummyClassifier(strategy='stratified', random_state=1)

baseline.fit(X, y)
ypred = baseline.predict(X)
print(accuracy_score(y,ypred))

print(confusion_matrix(y, ypred))

from sklearn.metrics import classification_report
print('\nClassification Report for Uniform Dummy Classifier\n')
print(classification_report(y, ypred, target_names=['None', 'Low', 'Medium','High']))

random.fit(X, y)
ypred = random.predict(X)
print(accuracy_score(y,ypred))
print(confusion_matrix(y, ypred))

from sklearn.metrics import classification_report
print('\nClassification Report Stratified Dummy Classifier\n')
print(classification_report(y, ypred, target_names=['None', 'Low', 'Medium','High']))

"""# **K Fold Cross Validation**"""

#Kfold validation results
#Define Empty lists to store the variables
mean_error = [];
std_error = [];
accuracy_values = [];
precision_values = [];
f1_values = [];
#Define the number of K fold splits
kf = KFold(n_splits=5)
#Define a list of K neighbours to be tested
k_range = [1,2,5,7,10,12,15,20,30,50,100,200]
for k in k_range:
    model = KNeighborsClassifier(n_neighbors=k,weights='uniform') 
    #Define the temp array
    temp = [];
    temp_accuracy = [];
    temp_f1 = [];
    temp_precision = [];
    for train, test in kf.split(X):
        #Fit the Xpoly data to the model
        model.fit(X[train],y[train])
        #Generate the Xpoly predictions
        ypred = model.predict(X[test])
        #Store the values in a temporary array where the mean value will be extracted
        temp_accuracy.append(accuracy_score(y[test],ypred))
        temp_f1.append(f1_score(y[test], ypred, average='weighted'))
        temp_precision.append(precision_score(y[test], ypred, average='weighted',zero_division=0))

    #Calculate the mean value from the temporary array for each k fold
    std_error.append(np.array(temp).std())
    accuracy_values.append(np.array(temp_accuracy).mean())
    precision_values.append(np.array(temp_precision).mean())
    f1_values.append(np.array(temp_f1).mean())

#Plot the figures
plt.figure()
plt.plot(k_range,accuracy_values,linewidth=3,color='red',label='Accuracy')
plt.plot(k_range,precision_values,linewidth=3,color='blue',label='Precision')
plt.plot(k_range,f1_values,linewidth=3,color='green',label='F1_Score')
plt.xlabel('K Neighbours')
plt.title("KNN Cross Validation vs Accuracy, Precision and F1_Score")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
plt.savefig("KNN_accurac_KNN_db_all_stations_9am.png")

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)
model = KNeighborsClassifier(n_neighbors=20,weights='uniform') 
model.fit(X_train, y_train)
ypred = model.predict(X_test)

print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test, ypred))
print(precision_score(y_test, ypred, average='weighted'))
print (f1_score(y_test, ypred, average='weighted'))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_test, ypred, target_names=['None', 'Low', 'Medium','High']))